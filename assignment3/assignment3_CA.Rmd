---
title: "Assignment 3: Task 1 - Cluster Analysis"
author: "Group 11"
date: "13.11.2018"
output: pdf_document 
---

```{r, warning = F, message = F, echo = F}
library(cluster)
library(tidyverse)
library(summarytools)
library(reshape2)
library(MASS)
library(fpc)
library(mclust)
library(class)
library(diceR)
```

\section{1. Problem Statement}
The presented data contains students' enrolment activites for 72 universities (observations), which is further subdivided into 13 faculties (variables). The task is to perform several cluster methods in order to identify for which universites students have a similar enrolment behavior. The analysis starts with some descriptive statistics, followed by a short comment regarding the assumtions. Afterwards hierarchical methods as well as non-hierarchical methods and others are applied. Finally, a conclusion regarding the best performing clustering method is made.

\section{2. Descriptive Statistics}
```{r, warning = F, message = F}
unistudis <- as.tibble(read.table("../data/unistudis.txt", header=T))

descr(unistudis[,-length(unistudis)], style = "rmarkdown", 
      stats = c("mean", "sd", "min", "q1", "med", "q3", "max", "pct.valid"))
```
First of all there are no missing values in the data, which is seen in the Pct.Valid row. Other than that the variables seem to differ in terms of mean and spread. FEB, FSS, FA, FPES, FKRS and FCL have relatively high mean values compared to the remaining ones. This means that students of the explicitly mentioned facultes on average enrl more frequently into language coureses. In terms of standard deviations FEB, FSS, FA, FPES, FKRS, FET, FCL and FS seem to have a high spread compared to the other faculties. So for these faculties students enrolment behavior differs strongly from university to university, whereas for other faculties it doesn't. 

```{r, warning = F, message = F, fig4, out.width = '50%'}
# Detecting Outliers
melt(unistudis[,-length(unistudis)]) %>% ggplot(aes(x = variable, y = value)) +
  geom_boxplot() +
  theme_light()

subset(unistudis, uniID == "16" | uniID == "46")
```

In terms of outliers one can clearly see that some are present. Especially observation 46 and 16 seem to be problematic. Whereas observation 46 has outlier values at variable FS and FPES, observation 16 has outlier values at variable FArch, FSS, FBE and FTRS.

\section{3. Assumptions}
Even though there are no explicit assumptions when using cluster algorithms one has to consider the fact that variables with a higher spread will have a higher importance in hierarchical cluster algorithms. This argumentation goes along with outlier values, as they might be clustered in a cluster containing only the outlier value. Therefore, observation 16 and 46 are removed before centering and standardizing the data.

```{r, warning = F, message = F}
# Remove observation 16 and 46
unistudis <- unistudis %>% filter(uniID != "16", uniID != "46")

# Standardize values
std_unistudis <- as.tibble(unistudis[,1:ncol(unistudis)-1] %>% scale(center = T, scale = T))
std_unistudis <- std_unistudis %>% mutate(uniID = unistudis$uniID) %>% dplyr::select(uniID, 1:13)
```

\section{4. Method and Interpretation}
\subsection{4.1 Hierarchical Methods}

The hierarchical methods applied in this section are single linkage, complete linkage, average linkage, centroid method and Ward's error sum of squares. The first step is to identify the correct amount of clusters based on either the visual analysis of the dendrograms or by investigating the drop in $R^2$. Analysing the dendrograms means examining the sizes of the changes in height in the dendrograms. A large change indicates the appropriate number of clusters. The authors decide to evaluate the dendrograms.

```{r}
# Calculate euclidean distance based on standardized values
dist_unistudis <- dist(std_unistudis[,-1])

# Apply hierarchical cluster methods
hclust_methods <-
  c("ward.D", "single", "complete", "average", "centroid")
hclust_results <- lapply(hclust_methods, function(m) hclust(dist_unistudis, m))
names(hclust_results) <- hclust_methods

# Plot dendrograms
par(mfrow = c(2,3))
hclust_dendro <- lapply(names(hclust_results), function(m) plot(hclust_results[[m]], 
                                                                main = m))
names(hclust_dendro) <- hclust_methods
par(mfrow = c(1,1))
```

The dendrograms of Ward's method and complete indicate a two cluster solution, whereas based on the remaining dendrograms on could also decide for more clusters. The authors decide to continue with a two cluster solution. \

Hence, the trees of each method are each cut into two clusters. To further evaluate whether the right amount of clusters is chosen the silhouette plots need to be analysed. On the other hand, silhouette plots not only provide information about the right amount of clusters chosen, but also give an indication about the degree of homogeneousity in each cluster. Silhouette values range from $-1$ to $1$, whereas values close to $-1$ indicate observations poorly classified and values close to $1$ vice versa. Observations with values close to $0$ are intermediate cases, which can be assigned to one or another cluster equally likely. 

```{r, results = 'hide', fig.keep = 'all'}
# Assign observations to two clusters
noclust <- 2
hclust_cutree <- mapply(cutree, hclust_results, noclust)

# Create silhouette plots
par(mfrow = c(2,3))
sapply(colnames(hclust_cutree), function(m) plot(silhouette(hclust_cutree[,m], 
                                                            dist_unistudis), main = m))
par(mfrow = c(1,1))
```

The silhouette plots show that the single linkage and centroid method cluster 69 in one cluster, which is a result of the chaining effect and gives basically no gain in information. The remaining models are better interpretalbe, as the relative frequency in the second cluster is higher. Nonetheless, silhouette scores of bigger than 0.4 are rare in each method, meaning that most of the observations are close to be intermediate cases. Therefore one might try to use non-hierarchical methods using input values from hierarchical methods to come to a better solution.

\subsection{4.2 Non-Hierarchical and Model Based Methods}

The non-hierarchical method used in this analysis is k-Means. As the result of k-Means is strongly dependent on the initial seeds, we are using random seeds as well as the results from each hierarchical cluster method as initial seeds. To evaluate the goodness of the methods the silhouette plots are again analysed.

```{r, results = 'hide', fig.keep = 'all'}
# Attach clusters to observations
std_unistudis_EXT <- as.tibble(cbind(std_unistudis[,-1], hclust_cutree))

# Calculate initials
initials <- lapply(hclust_methods, 
                   function(m) aggregate(std_unistudis[,-1], 
                                         list(as.vector(t(std_unistudis_EXT[,m]))), mean))
# k-Means with initials
clus_kmeans <- lapply(seq(1:length(hclust_methods)), 
                      function(m) kmeans(std_unistudis[,-1], centers = initials[[m]][,-1]))

# k-means with random seeds
set.seed(1)
clus_kmeans[[length(clus_kmeans)+1]] <- kmeans(std_unistudis[,-1], centers = noclust)

# Create silhouette plots
par(mfrow = c(2,3))
pnames <- paste("k-Means:", c(hclust_methods, "random"))
lapply(seq(1:length(clus_kmeans)), 
       function(m) plot(silhouette(clus_kmeans[[m]]$cluster, dist_unistudis), 
                        main = pnames[m]))
```
First of all each method clusters approximataly the same observations in each cluster. Other than that the silhouette plots of k-Means with Ward and average linkage seeding seem to be the best k-Means cluster solutions. This can also be validated when comparing the average silhouette width. K-Means with complete seeding performs bad in specifying the second cluster, whereas the remaining methods have difficulties in specifying the first cluster. 

\section{5. Alternative solutions}

```{r out.width = '50%'}
# k-Mediods (Partitioning Around Mediods)
unistudis_all <- as.tibble(read.table("../data/unistudis.txt", header=T))
std_unistudis_all <- as.tibble(unistudis_all[,1:ncol(unistudis_all)-1] %>% scale(center = T, scale = T))
std_unistudis_all <- std_unistudis_all %>% mutate(uniID = unistudis_all$uniID) %>% dplyr::select(uniID, 1:13)
dist_unistudis_all <- dist(std_unistudis_all[,-1])

clus_pam <- pam(std_unistudis_all[,-1], noclust)

# Model based clustering (selection based on BIC)
clus_mod <- Mclust(std_unistudis[,-1], G = 2:9)

par(mfrow = c(1,2))
plot(silhouette(clus_pam$cluster, dist_unistudis_all))
plot(silhouette(clus_mod$classification, dist_unistudis))

# Cluster ensemble
CC <- consensus_cluster(std_unistudis, nk = 2:4, p.item = 0.8, reps = 5,
                        algorithms = c( "pam", "diana", "hc", "gmm"))

pam.2 <- CC[, , "PAM_Euclidean", "2", drop = FALSE]
cm <- consensus_matrix(pam.2)
hm <- graph_heatmap(pam.2)
ccomp <- consensus_evaluate(std_unistudis, CC, plot = FALSE)
```
Additionally, Partitioning Around Mediods (PAM) and model-based clustering was explored. PAM is supposed to be less sensitive to outliers compared to k-means, which is why we applied it to the full dataset. The silhouette plot did not show a homogenous within cluster structure. Another approach includes the model-based clustering which tries to estimate the distribution of cluster segments and maximize the probability that a sample comes from one distribution. This is also called soft partitioning, while the other approaches followed hard partitioning.
We also explored ensemble methods as way of directly comparing different algorithms and cluster sizes. Specifically, we compared PAM and two hierarchical clustering methods, DIANA, HC and one model-based approach, GMM. DIANA is a divisive clustering algorithm while HC takes an agglomarative approach. In 5 rounds of applying each algorithm to bootstrapped subsamples of the data a consensus within the cluster assignment is reached. This can be visualized per algorithm and cluster size in a NxN Consensus Matrix, with N = \# samples. The matrix values are within a $[0,1]$ boundary with 1 indicating agreement across all iterations of sample assignment. The heatmap output is enclosed in the appendix. It showed high agreement within DIANA and PAM, especially for cluster size = 2. This means, that the clustering solution is unambigious. This takes a different approach in evaluation, as we are not considering the silhouette plot width but instead the agreement within an algorithm in regard of the cluster assignment. 

The evaluation with respect to compactness and seperability (see appendix) shows that the algorithms produce equally compact clusters but DIANA and PAM generate solutions which are more separable. This analysis supports our previously chosen solution of two clusters. The model based approach seems to be not optimal for this task, as it generates clusters very different from the other algorithms. This might be caused by the strong assumptions of an underlying gaussian distribution of each cluster is not satisfied. 

\section{6. Conclusion}
The performed cluster analysis consisted of the comparison of different hierarchical approaches and their dendogram and silhouette plots. The results were fed into k-means and k-mediods as one of several methods for initializing the cluster seeds. Additionally, model based and ensemble methods were explored. The best solution in terms of within cluster connectivity, consensus of the algorithm and silhouette plots was achieved by the k-means method using two clusters and initial seeds from the prior hierarchical solution.


\newpage
\section{Appendix}
```{r}
ccomp$ii


diana.2 <- CC[, , "DIANA_Euclidean", "2", drop = FALSE]
hc.2 <- CC[, , "HC_Euclidean", "2", drop = FALSE]
gmm.2 <- CC[, , "GMM", "2", drop = FALSE]
hm.1 <- graph_heatmap(hc.2)
hm.2 <- graph_heatmap(diana.2)
gmm.2 <- graph_heatmap(gmm.2)
```
















